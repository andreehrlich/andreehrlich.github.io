<!DOCTYPE html>
<head>
  <meta charset="utf-8">
  <title>Andre Ehrlich</title>
  <link href="css/style.css" rel="stylesheet">
  <script src="js/jQuery.js"></script>
</head>
<body>
  <div id="main">
    <div id="header">
      <h1>Andre Ehrlich</h1>
    </div>
    <div id="intro">

      <p>I'm currently a PhD student in the Department of Statistics at the
      Athens University of Economics & Business, supervised by Nikos Demiris,
      and co-supervised by Petros Dellaportas. I'm interested in understanding
      biomedical data via learning algorithms that operate on functional
      distributions. I am investigating how well Bayesian Gaussian Processes &
      Neural Diffusion Processes can (1) perform inference on dynamical
      epidemic models characterized by stochastic ordinary differential
      equations, and (2) extrapolate long-term survival probabilities for
      cancer data.</p>
      <img source="./assets/profile.png" width="100" height="100">
    </div>
    <div id="tutorials">
      <h2>Tutorials</h2>
      <ul>
        <li>Non-Parametric Estimation</li>
        <li>Individualized Treatment Rule Estimation for Survival Data</li>
        <li>Bayesian Gaussian Process in Stan</li>
      </ul>
    </div>
    <div id="working-examples">
      <h2>Working Examples</h2>
    </div>
  </div>
    <div id="work-exp">
      <h2>Industry Experience</h2>
      <ul>
        <li> Technical Product Manager & Data Scientist in Maritime Shipping Computing. </li>
        <li> </li>
      </ul>
    </div>
  </div>
</body>
</html>
<!-- <body> -->
<!-- <img src="images/prof_pic_v4_cropped_v2.jpg" alt="Profile Pic" width="230" height="185.42" align="right"> -->
<!-- <h1>Xingyou (Richard) Song</h1> -->
<!-- <p>I am currently a Senior Research Scientist at <a href="https://research.google/teams/brain/">Google DeepMind</a>, working on a variety of topics, including Automated Machine Learning, Reinforcement Learning/Robotics, and Transformers. </p> -->
<!-- <p> Previously I have worked/interned at <a href="https://openai.com/">OpenAI</a>, <a href="https://www.citadelsecurities.com/">Citadel Securities</a>, and <a href="https://www.microsoft.com/en-us/research/lab/microsoft-research-asia/>Microsoft Research Asia">Microsoft Research Asia</a>. </p> -->
<!-- <p style="text-align:left"> -->
<!--   <a href="mailto:xingyousong@google.com">Email</a> &nbsp;/&nbsp; -->
<!--   <a href="https://dblp.uni-trier.de/pers/s/Song:Xingyou.html">DBLP</a> &nbsp;/&nbsp; -->
<!--   <a href="https://scholar.google.com/citations?user=GnpHmO8AAAAJ&amp;hl=en">Google Scholar</a> &nbsp;/&nbsp; -->
<!--   <a href="https://www.linkedin.com/in/xingyou-song-355629a1/">Linkedin</a> &nbsp;/&nbsp; -->
<!--   <a href="https://github.com/xingyousong">Github</a> &nbsp;/&nbsp; -->
<!--   <a href="https://twitter.com/XingyouSong/">Twitter</a> &nbsp;/&nbsp; -->
<!--   <a href="data/CV.pdf">CV</a> -->
<!-- </p> -->
<!---->
<!-- <h2>Open Source Contributions</h2> -->
<!-- <ul> -->
<!---->
<!-- <li> -->
<!-- <div class="title">Open Source Vizier: Distributed Infrastructure and API for Reliable and Flexible Blackbox Optimization</div> -->
<!-- <div>Xingyou Song, Sagi Perel, Chansoo Lee, Greg Kochanski, Daniel Golovin</div> -->
<!-- <div>Automated Machine Learning Conference (AutoML-Conf) Systems Track 2022</div> -->
<!-- <b><u>Paper:</u></b> -->
<!-- <a href="https://arxiv.org/abs/2207.13676">[arXiv]</a> -->
<!-- <a href="https://ai.googleblog.com/2023/02/open-source-vizier-towards-reliable-and.html">[Google AI Blog]</a> -->
<!-- <a href="https://openreview.net/forum?id=SfIRlTSUxc">[AutoML-Conf]</a> -->
<!-- <a href="https://proceedings.mlr.press/v188/song22a.html">[PMLR]</a> -->
<!-- <a href="data/oss_vizier_poster.pdf">[Poster]</a> -->
<!-- &emsp;&emsp;&emsp;&emsp;<b><u>Code:</u></b> -->
<!-- <a href="https://github.com/google/vizier">[Github]</a> -->
<!-- <a href="https://pypi.org/project/google-vizier/">[PyPI]</a> -->
<!-- <a href="https://oss-vizier.readthedocs.io">[Docs]</a> -->
<!-- <br> -->
<!-- <b><u>External Media/References:</u></b> -->
<!-- <a href="https://ai.googleblog.com/2023/02/google-research-2022-beyond-algorithmic.html">[Google Research 2022 Algorithms]</a> -->
<!-- <a href="https://github.com/google-research/tuning_playbook/">[Deep Learning Tuning Playbook]</a> -->
<!-- <a href="https://www.marktechpost.com/2023/02/04/google-ai-open-sources-vizier-a-standalone-python-package-designed-for-managing-and-optimizing-machine-learning-experiments-at-scale/">[MarkTechPost]</a> -->
<!-- <a href="https://wandb.ai/vincenttu/blog_posts/reports/ChatGPT-the-Catalyst--VmlldzozNDg1Nzc2">[Weights &amp; Biases]</a> -->
<!-- <a href="https://thesequence.substack.com/p/the-chatgpt-challengers">[The Sequence]</a> -->
<!-- <a href="https://open.substack.com/pub/deeplearningweekly/p/deep-learning-weekly-issue-287?utm_campaign=post&amp;utm_medium=web">[Deep Learning Weekly]</a> -->
<!-- <a href="https://analyticsindiamag.com/google-vizier-is-now-open-source-and-thats-great-news/">[Analytics India Magazine]</a> -->
<!-- <a href="https://electronicsmith.com/web-stories/google-ai-open-sources-vizier/">[Electronic Smith]</a> -->
<!-- <a href="https://www.ghacks.net/2023/02/11/google-ai-open-sources-vizier">[gHacks]</a> -->
<!-- <a href="https://youtu.be/TOo-HnjjuhU">[ML News by Yannic Kilcher (Video)]</a> -->
<!-- <a href="https://webbigdata.jp/post-17645/">[WebBigdata (Japanese)]</a> -->
<!-- <a href="https://randomaccessnoticias.com/inteligencia-artificial/hacia-una-optimizacion-confiable-y-flexible-de-hiperparametros-y-blackbox-google-ai-blog/">[RandomAccess (Spanish)]</a> -->
<!-- <br> -->
<!-- <b><u>Presentations:</u></b> -->
<!-- <a href="data/oss_vizier_slides.pdf">[Slides]</a> -->
<!-- <a href="https://youtu.be/b5hemgM16tM">[AutoML-Conf 2022 Video]</a> -->
<!-- <a href="https://youtu.be/Ya_V5isGdG8">[AutoML Seminar Video]</a> -->
<!-- <a href="https://youtu.be/Xpdn_9uPEZY?si=WhtGZ7B01-BEV_t8">[AutoML-Conf 2023 Tutorial Video]</a> -->
<!-- </li> -->
<!---->
<!-- <li> -->
<!-- <div class="title">The Vizier Gaussian Process Bandit Algorithm</div> -->
<!-- <div>Xingyou Song, Qiuyi Zhang, Chansoo Lee, Emily Fertig, Tzu-Kuo Huang, Lior Belenkil, Greg Kochanski, Setareh Ariafar, Srinivas Vasudevan, Sagi Perel, Daniel Golovin</div> -->
<!-- <div>Google DeepMind Technical Report 2024</div> -->
<!-- <b><u>Paper:</u></b> -->
<!-- <a href="https://arxiv.org/abs/2408.11527">[arXiv]</a> -->
<!-- <br> -->
<!-- <b><u>External Media/References:</u></b> -->
<!-- <a href="https://m.chinaz.com/2024/0823/1637848.shtml">[ChinaZ]</a> -->
<!-- <a href="https://hub.baai.ac.cn/paper/bf5af29c-6e25-4206-b9f9-f88b4d9d4331">[BAAI]</a> -->
<!-- <a href="https://www.turingpost.com/p/fod64">[TuringPost]</a> -->
<!-- <a href="https://github.com/dair-ai/ML-Papers-of-the-Week/tree/main?tab=readme-ov-file#top-ml-papers-of-the-week-august-19---august-25---2024">[DAIR.AI]</a> -->
<!-- <a href="https://nlp.elvissaravia.com/p/top-ml-papers-of-the-week-ea5">[NLP Newsletter]</a> -->
<!-- <a href="https://www.aimodels.fyi/papers/arxiv/vizier-gaussian-process-bandit-algorithm">[AIModels.FYI]</a> -->
<!-- <a href="https://odsc.medium.com/odscs-ai-weekly-recap-week-of-august-30th-37921c9d13c3">[Open Data Science]</a> -->
<!-- <a href="https://www.appypie.com/blog/top-ml-papers-of-the-week-august-19-august-25-2024">[Appy Pie]</a> -->
<!-- <br> -->
<!-- <b><u>Presentations:</u></b> -->
<!-- <a href="data/vizier_gp_bandit.pdf">[Slides]</a> -->
<!-- <a href="https://event.baai.ac.cn/activities/834">[BAAI Talk]</a> -->
<!-- </li> -->
<!---->
<!-- </ul> -->
<!---->
<!-- <h2>Preprints and Publications</h2>  -->
<!-- <ul> -->
<!---->
<!-- <li> -->
<!-- <div class="title">Predicting from Strings: Language Model Embeddings for Bayesian Optimization</div> -->
<!-- <div>Tung Nguyen, Qiuyi Zhang, Bangding Yang, Chansoo Lee, Jorg Bornschein, Yingjie Miao, Sagi Perel, Yutian Chen, Xingyou Song</div> -->
<!-- <div>Google DeepMind Technical Report 2024</div> -->
<!-- <b><u>Paper:</u></b> -->
<!-- <a href="https://arxiv.org/abs/2410.10190">[arXiv]</a> -->
<!-- <a href="https://github.com/google-research/optformer/tree/main/optformer/embed_then_regress">[Code]</a> -->
<!-- <br> -->
<!-- <b><u>External Media/References:</u></b> -->
<!-- <a href="https://www.marktechpost.com/2024/10/19/embed-then-regress-a-versatile-machine-learning-approach-for-bayesian-optimization-using-string-based-in-context-regression/">[MarkTechPost]</a> -->
<!-- </li> -->
<!---->
<!-- <li> -->
<!-- <div class="title">Position: Leverage Foundational Models for Black-Box Optimization</div> -->
<!-- <div>Xingyou Song, Yingtao Tian, Robert Tjarko Lange, Chansoo Lee, Yujin Tang, Yutian Chen</div> -->
<!-- <div>International Conference on Machine Learning (ICML) 2024</div> -->
<!-- <b><u>Paper:</u></b> -->
<!-- <a href="https://arxiv.org/abs/2405.03547">[arXiv]</a> -->
<!-- <a href="data/bbo_position_poster.pdf">[Poster]</a> -->
<!-- <br> -->
<!-- <b><u>External Media/References:</u></b> -->
<!-- <a href="https://www.turingpost.com/p/fod52">[TuringPost]</a> -->
<!-- <a href="https://www.eye-on.ai/ai-articles/gf6p33bptlzm33p-kxsw9-5jlhh-44dxp-h9nbn-gj5zf-cja2x-r35j4-ndyzh-n33hp-5lw6m-c5xxc-7njj7-xhwe4-p82pz-sszl2-twjcd-etta3-dpjej-psgmj-x9rtl-xnx22-ax2tp-9fp4n-tygla-7dmc5-9r4ft-mpshx-4yb27-glkw3-dhfn2">[Eye on AI]</a> -->
<!-- <a href="https://transferlab.ai/pills/2024/position-leverage-foundational-models-for-black-box-optimization/">[TransferLab]</a> -->
<!-- </li> -->
<!--    -->
<!-- <li> -->
<!-- <div class="title">OmniPred: Language Models as Universal Regressors</div> -->
<!-- <div>Xingyou Song, Oscar Li, Chansoo Lee, Bangding (Jeffrey) Yang, Daiyi Peng, Sagi Perel, Yutian Chen</div> -->
<!-- <b><u>Paper:</u></b> -->
<!-- <a href="https://arxiv.org/abs/2402.14547">[arXiv]</a> -->
<!-- <a href="https://github.com/google-research/optformer/tree/main/optformer/omnipred">[Code]</a> -->
<!-- <a href="data/omnipred_poster.pdf">[Poster]</a> -->
<!-- <br> -->
<!-- <b><u>External Media/References:</u></b> -->
<!-- <a href="https://huggingface.co/papers?date=2024-02-23">[HuggingFace Daily Papers]</a> -->
<!-- <a href="https://youtu.be/U6B6ms3PtKg?si=Ro_qViv-6k8N-HQA">[arXiv Papers (Video)]</a> -->
<!-- <a href="https://www.marktechpost.com/2024/03/02/meet-omnipred-a-machine-learning-framework-to-transform-experimental-design-with-universal-regression-models/">[MarkTechPost]</a> -->
<!-- <a href="https://promptengineering.org/language-models-as-universal-regressors/">[Prompt Engineering Institute]</a> -->
<!-- <a href="https://bnnbreaking.com/world/us/omnipred-revolutionizes-predictive-analytics-with-universal-regression-models-surpassing-traditional-methods">[BNN Breaking]</a> -->
<!-- <a href="https://multiplatform.ai/unlocking-potential-omnipred-redefines-experimental-design-with-universal-regression-models/">[MultiPlatform AI]</a> -->
<!-- <a href="https://linnk.ai/insight/machine-learning/OmniPred-Language-Models-as-Universal-Regressors-gAtiZQ8N/">[Linnk]</a> -->
<!-- <a href="https://youtu.be/lsvISZJvFv8?si=s_gtmU8Vk69loCqE">[Tunadorable Papers]</a> -->
<!-- <a href="https://blog.csdn.net/xixiaoyaoww/article/details/136438700">[CSDN (Chinese)]</a> -->
<!-- <br> -->
<!-- <b><u>Talks:</u></b> -->
<!-- <a href="data/omnipred_talk.pdf">[Slides]</a> -->
<!-- <a href="https://youtu.be/fv-cK9LgQmk?si=AWfrDqATvB6saTG4">[AutoML Seminar]</a> -->
<!-- <a href="https://mlcollective.org/dlct/">[ML Collective]</a> -->
<!---->
<!-- </li> -->
<!---->
<!-- <li> -->
<!-- <div class="title">Hardness of Low Rank Approximation of Entrywise Transformed Matrix Products</div> -->
<!-- <div>(a) Tamas Sarlos, Xingyou Song, David Woodruff, Qiuyi Zhang</div> -->
<!-- <div>Neural Information Processing Systems (NeurIPS) 2023</div> -->
<!-- <a href="https://arxiv.org/abs/2311.01960">[arXiv]</a> -->
<!-- <a href="https://neurips.cc/virtual/2023/poster/71970">[NeurIPS]</a> -->
<!-- </li> -->
<!---->
<!-- <li> -->
<!-- <div class="title">Discovering Adaptable Symbolic Algorithms from Scratch</div> -->
<!-- <div>Stephen Kelly, Daniel S. Park, Xingyou Song, Mitchell McIntire, Pranav Nashikkar, Ritam Guha, Wolfgang Banzhaf, Kalyanmoy Deb, Vishnu Naresh Boddeti, Jie Tan, Esteban Real</div> -->
<!-- <div>International Conference on Intelligent Robots and Systems (IROS) 2023 <span style="color:red"><b>(Best Overall Paper Finalist, Top 12/2760 of Submissions)</b></span></div> -->
<!-- <b><u>Paper:</u></b> -->
<!-- <a href="http://arxiv.org/abs/2307.16890">[arXiv]</a> -->
<!-- <a href="https://youtu.be/sEFP1Hay4nE">[Video]</a> -->
<!-- <a href="data/arz_poster.pdf">[Poster]</a> -->
<!-- <br> -->
<!-- <b><u>External Media/References:</u></b> -->
<!-- <a href="https://huggingface.co/papers?date=2023-08-01">[HuggingFace Daily Papers]</a> -->
<!---->
<!-- </li> -->
<!--    -->
<!-- <li> -->
<!-- <div class="title">Towards Learning Universal Hyperparameter Optimizers with Transformers</div> -->
<!-- <div>Yutian Chen, Xingyou Song, Chansoo Lee, Zi Wang, Qiuyi Zhang, David Dohan, Kazuya Kawakami, Greg Kochanski, Arnaud Doucet, Marc'aurelio Ranzato, Sagi Perel, Nando de Freitas</div> -->
<!-- <div>Neural Information Processing Systems (NeurIPS) 2022</div> -->
<!-- <b><u>Paper:</u></b> -->
<!-- <a href="https://arxiv.org/abs/2205.13320">[arXiv]</a> -->
<!-- <a href="https://github.com/google-research/optformer">[Code]</a> -->
<!-- <a href="https://ai.googleblog.com/2022/08/optformer-towards-universal.html">[Google AI Blog]</a> -->
<!-- <a href="data/optformer_poster.pdf">[Poster]</a> -->
<!-- <br> -->
<!-- <b><u>External Media/References:</u></b> -->
<!-- <a href="https://www.tensorflow.org/text/tutorials/transformer/">[Tensorflow Transformer Tutorial]</a> -->
<!-- <a href="https://vevesta.substack.com/p/google-optformer">[Vevesta Labs]</a> -->
<!-- <a href="https://www.accenture.com/us-en/blogs/federal-viewpoints/baseline-october-2022">[Accenture Federal Viewpoints]</a> -->
<!-- <a href="https://www.marktechpost.com/2022/08/22/google-ai-introduces-optformer-the-first-transformer-based-framework-for-hyperparameter-tuning/">[MarkTechPost]</a> -->
<!-- <a href="https://www.theee.ai/2022/08/22/19653-optformer-towards-universal-hyperparameter-optimisation-with-transformers/">[The Evolving Enterprise]</a> -->
<!-- <a href="https://logictechworld.com/2022/08/18/towards-the-universal-optimization-of-hyperparameters-with-transformers/">[LogicTech World]</a> -->
<!-- <a href="https://www.analyticsvidhya.com/blog/2022/09/hyperparameter-optimization-using-optformer/">[Analytics Vidhya]</a> -->
<!-- <a href="https://sameog.com/in-the-direction-of-common-hyperparameter-optimization-with-transformers/">[Same OG]</a> -->
<!-- <a href="https://technewscrypt.com/towards-universal-hyperparameter-optimization-with-transformers/">[Tech News Crypt]</a> -->
<!-- <a href="https://theperfectech.com/in-the-direction-of-common-hyperparameter-optimization-with-transformers/">[The Perfectech]</a> -->
<!-- <a href="https://ianoticias.com/hacia-la-optimizacion-universal-de-hiperparametros-con-transformadores/">[IANoticas (Spanish)]</a> -->
<!-- <a href="https://mundoxdigital.com/hacia-la-optimizacion-universal-de-hiperparametros-con-transformadores/">[MundoXDigital (Spanish)]</a> -->
<!-- <a href="https://digizom.com/hacia-la-optimizacion-universal-de-hiperparametros-con-transformadores/">[Digizom (Spanish)]</a> -->
<!-- <a href="https://www.datalearner.com/ai-resources/ai-paper-news/1051653740336238">[DataLearner (Chinese)]</a> -->
<!-- <a href="https://hub.baai.ac.cn/view/19955">[HubAI (Chinese)]</a> -->
<!-- <a href="https://www.aitimes.kr/news/articleView.html?idxno=25822">[AI Times (Korean)]</a> -->
<!-- <a href="https://webbigdata.jp/ai/post-14380">[Web Big Data (Japanese)]</a> -->
<!-- <br> -->
<!-- <b><u>Slides:</u></b> -->
<!-- <a href="https://rosanneliu.com/dlctfs/dlct_220902.pdf">[OptFormer Slides]</a> -->
<!-- <a href="data/llm_optimization_automl2023.pdf">[LLMs for Optimization Slides]</a> -->
<!-- <br> -->
<!-- <b><u>Talk Videos:</u></b> -->
<!-- <a href="https://youtu.be/C7nMCMolo08">[ML Collective]</a> -->
<!-- <a href="https://youtu.be/dt7yn_kSrDI">[AutoML Seminar]</a> -->
<!-- <a href="https://youtu.be/vcfEG6u1bwk?si=Mbdig5y1Mn3En13F">[AutoML-Conf 2023 Tutorial]</a> -->
<!---->
<!-- </li> -->
<!---->
<!-- <li> -->
<!-- <div class="title">Automated Reinforcement Learning (AutoRL): A Survey and Open Problems</div> -->
<!-- <div>(a) Jack Parker-Holder<span style="background-position: right top;">*</span>, Raghu Rajan<span style="background-position: right top;">*</span>, Xingyou Song<span style="background-position: right top;">*</span>, André Biedenkapp, Yingjie Miao, Theresa Eimer, Baohe Zhang, Vu Nguyen, Roberto Calandra, Aleksandra Faust, Frank Hutter, Marius Lindauer</div> -->
<!-- <div>Journal of Artificial Intelligence Research (JAIR) 2022</div> -->
<!-- <div>Automated Machine Learning Conference (AutoML-Conf) Journal Track 2023 </div> -->
<!-- <div> -->
<!-- <a href="https://arxiv.org/abs/2201.03916">[arXiv]</a> -->
<!-- <a href="https://jair.org/index.php/jair/article/view/13596">[JAIR]</a> -->
<!-- <a href="https://youtu.be/9FDqUsByRiQ">[Tutorial Video]</a> -->
<!-- <a href="https://automl.cc/wp-content/uploads/2022/07/AutoRL-Tutorial-@-AutoML.pdf">[Tutorial Slides]</a> -->
<!-- </div> -->
<!-- </li> -->
<!---->
<!-- <li> -->
<!-- <div class="title">Differentiable Architecture Search for Reinforcement Learning</div> -->
<!-- <div>Yingjie Miao<span style="background-position: right top;">*</span>, Xingyou Song<span style="background-position: right top;">*</span>, John D. Co-Reyes, Daiyi Peng, Summer Yue, Eugene Brevdo, Aleksandra Faust</div> -->
<!-- <div>Automated Machine Learning Conference (AutoML-Conf) 2022</div> -->
<!-- <div> -->
<!-- <a href="https://arxiv.org/abs/2106.02229">[arXiv]</a> -->
<!-- <a href="https://openreview.net/forum?id=BMx6eaBUe5">[AutoML-Conf]</a> -->
<!-- <a href="https://proceedings.mlr.press/v188/miao22a.html">[PMLR]</a> -->
<!-- <a href="data/rl_darts_slides.pdf">[Slides]</a> -->
<!-- <a href="https://youtu.be/S39sWHJp60g">[Video]</a> -->
<!-- <a href="https://github.com/google/brain_autorl/tree/main/rl_darts">[Code]</a> -->
<!-- <a href="data/rl_darts_poster.pdf">[Poster]</a> -->
<!-- </div> -->
<!-- </li> -->
<!--   -->
<!-- <li> -->
<!-- <div class="title">ES-ENAS: Efficient Evolutionary Optimization for Large Hybrid Search Spaces</div> -->
<!-- <div>Xingyou Song, Krzysztof Choromanski, Jack Parker-Holder, Yunhao Tang, Qiuyi Zhang, Daiyi Peng, Deepali Jain, Wenbo Gao, Aldo Pacchiano, Tamas Sarlos, Yuxiang Yang</div> -->
<!-- <div>International Conference on Learning Representations (ICLR) 2020, Workshop on Neural Architecture Search</div> -->
<!-- <div> -->
<!-- <a href="https://arxiv.org/abs/2101.07415">[arXiv]</a> -->
<!-- <a href="https://github.com/google-research/google-research/tree/master/es_enas">[Code]</a> -->
<!-- <a href="data/chromatic_poster.pdf">[Poster (Old)]</a> -->
<!-- </div> -->
<!-- </li> -->
<!---->
<!-- <li> -->
<!-- <div class="title">Unlocking Pixels for Reinforcement Learning via Implicit Attention</div> -->
<!-- <div>Krzysztof Choromanski<span style="background-position: right top;">*</span>, Deepali Jain<span style="background-position: right top;">*</span>, Wenhao Yu<span style="background-position: right top;">*</span>, Xingyou Song, Jack Parker-Holder, Tingnan Zhang, Valerii Likhosherstov, Aldo Pacchiano, Anirban Santara, Yunhao Tang, Jie Tan, Adrian Weller.</div> -->
<!-- <div> -->
<!-- <a href="https://arxiv.org/abs/2102.04353">[arXiv]</a> -->
<!-- <a href="https://sites.google.com/corp/view/implicitattention">[Site]</a> -->
<!-- </div> -->
<!-- </li> -->
<!---->
<!-- <li> -->
<!-- <div class="title">Sub-Linear Memory: How to Make Performers SLiM</div> -->
<!-- <div>Valerii Likhosherstov, Krzysztof Choromanski, Jared Davis, Xingyou Song, Adrian Weller</div> -->
<!-- <div>Neural Information Processing Systems (NeurIPS) 2021</div> -->
<!-- <div> -->
<!-- <a href="https://arxiv.org/abs/2012.11346">[arXiv]</a> -->
<!-- <a href="https://github.com/google-research/google-research/tree/master/performer/models/slim_performer">[Code]</a> -->
<!-- </div> -->
<!-- </li> -->
<!---->
<!-- <li> -->
<!-- <div class="title">Debiasing First-order Heuristic for Approximate Bi-level Optimization</div> -->
<!-- <div>Valerii Likhosherstov<span style="background-position: right top;">*</span>, Xingyou Song<span style="background-position: right top;">*</span>, Krzysztof Choromanski, Jared Davis, Adrian Weller</div> -->
<!-- <div>Interntional Conference on Machine Learning (ICML) 2021</div> -->
<!-- <div> -->
<!-- <a href="https://arxiv.org/abs/2106.02487">[arXiv]</a> -->
<!-- <a href="https://icml.cc/virtual/2021/poster/9027">[ICML]</a> -->
<!-- <a href="https://github.com/xingyousong/ufom">[Code]</a> -->
<!-- </div> -->
<!-- </li> -->
<!---->
<!-- <li> -->
<!-- <div class="title">Rethinking Attention with Performers</div> -->
<!-- <div>Krzysztof Choromanski<span style="background-position: right top;">*</span>, Valerii Likhosherstov<span style="background-position: right top;">*</span>, David Dohan<span style="background-position: right top;">*</span>, Xingyou Song<span style="background-position: right top;">*</span>, Andreea Gane<span style="background-position: right top;">*</span>, Tamas Sarlos<span style="background-position: right top;">*</span>, Peter Hawkins<span style="background-position: right top;">*</span>, Jared Davis<span style="background-position: right top;">*</span>, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, Adrian Weller</div> -->
<!-- <div>International Conference on Learning Representations (ICLR) 2021 <span style="color:red"><b>(Oral, Top 2% of Submissions)</b></span></div> -->
<!-- <div> -->
<!-- <b><u>Paper:</u></b> -->
<!-- <a href="https://arxiv.org/abs/2009.14794">[arXiv]</a> -->
<!-- <a href="https://openreview.net/forum?id=Ua6zuk0WRH">[ICLR]</a> -->
<!-- <a href="https://ai.googleblog.com/2020/10/rethinking-attention-with-performers.html">[Google AI Blog]</a> -->
<!-- <a href="data/performer_ppt.pdf">[Slides]</a> -->
<!-- &emsp;&emsp;&emsp;&emsp;<b><u>Code:</u></b> -->
<!-- <a href="https://github.com/google-research/google-research/tree/master/protein_lm">[Protein LM Code]</a> -->
<!-- <a href="https://github.com/google-research/google-research/tree/master/performer">[Performer Code]</a> -->
<!-- <br> -->
<!-- <b><u>External Media/Blogposts:</u></b> -->
<!-- <a href="https://www-forbes-com.cdn.ampproject.org/c/s/www.forbes.com/sites/robtoews/2023/09/03/transformers-revolutionized-ai-what-will-replace-them/amp/">[Forbes]</a> -->
<!-- <a href="https://ai.googleblog.com/2023/02/google-research-2022-beyond-algorithms.html">[Google Research 2022]</a> -->
<!-- <a href="https://iclr-blog-track.github.io/2022/03/25/Looking-at-the-Performer-from-a-Hopfield-point-of-view/">[Hopfield Networks (ICLR Blog Track)]</a> -->
<!-- <a href="https://people.idsia.ch/~juergen/fast-weight-programmer-1991-transformer.html/">[Schmidhuber AI Blog]</a> -->
<!-- <a href="https://huggingface.co/blog/long-range-transformers/">[HuggingFace]</a> -->
<!-- <a href="keramitas.io/2020/12/02/scaling-transformers-perform-at-your-best.html">[Kerasmitas]</a> -->
<!-- <a href="https://www.iarai.ac.at/news/rethinking-attention-with-performers/">[IARAI]</a> -->
<!-- <a href="https://ml-jku.github.io/blog-post-performer/">[Hopfield Networks]</a> -->
<!-- <a href="https://www.infoq.com/news/2020/11/google-attention-performer/">[InfoQ]</a> -->
<!-- <a href="https://towardsdatascience.com/from-transformers-to-performers-approximating-attention-69c88af0b11f/">[TowardsDataScience]</a> -->
<!-- <a href="https://analyticsindiamag.com/transformers-attention-google-introduces-performers/">[AIM]</a> -->
<!-- <a href="https://www.eyerys.com/articles/news/googles-performer-aims-solve-resource-hungry-transformer-architecture/">[Eyerys]</a> -->
<!-- <a href="https://teddykoker.com/2020/11/performers/">[TeddyKoker]</a> -->
<!-- <a href="https://exbulletin.com/tech/495730/">[ExBulletin]</a> -->
<!-- <a href="https://www.marktechpost.com/2020/10/25/google-ai-introduces-performer-a-generalized-attention-framework-based-on-the-transformer-architecture/">[MarkTechPost]</a> -->
<!-- <a href="https://syncedreview.com/2020/10/02/google-cambridge-deepmind-alan-turing-institutes-performer-transformer-slashes-compute-costs/">[Synced (v2)]</a> -->
<!-- <a href="https://syncedreview.com/2020/07/31/applying-linearly-scalable-transformers-to-model-longer-protein-sequences/">[Synced (v1)]</a> -->
<!-- <a href="https://venturebeat.com/2020/06/08/googles-performer-ai-architecture-could-advance-protein-analysis-and-cut-compute-costs/">[VentureBeat]</a> -->
<!-- <a href="https://www.linkresearcher.com/theses/ebd1472f-a6b3-49c7-ae34-a25eedd0e08e/">[LinkResearcher (Chinese)]</a> -->
<!-- <a href="https://zhuanlan.zhihu.com/p/280864164/">[Zhuanlan (Chinese)] -->
<!-- </a><a href="https://kexue.fm/archives/7921">[KeXue (Chinese)] -->
<!-- </a><a href="https://machinelearning.co.il/7799/rethinking-attention-with-performers-%d7%a1%d7%a7%d7%99%d7%a8%d7%94/">[MDLI (Hebrew)]</a> -->
<!-- <a href="https://probml.github.io/pml-book/book1.html">[MIT Press (Sec 15.6)]</a> -->
<!-- <a href="https://ai.googleblog.com/2021/01/google-research-looking-back-at-2020.html">[Google Research 2020]</a> -->
<!-- <br> -->
<!-- <b><u>Forums:</u></b> -->
<!-- <a href="https://news.ycombinator.com/item?id=24878116">[Hacker News (YCombinator)]</a> -->
<!-- <a href="https://www.reddit.com/r/MachineLearning/comments/jiich1/d_paper_explained_rethinking_attention_with/">[Paper Explained (Reddit)]</a> -->
<!-- <a href="https://www.reddit.com/r/MachineLearning/comments/kml2fx/p_performers_the_kernel_trick_random_fourier/">[FAVOR Explained (Reddit)]</a> -->
<!-- <br> -->
<!-- <b><u>Talk Videos:</u></b> -->
<!-- <a href="https://youtu.be/NzlrQgb_KZ4">[LightOnAI]</a> -->
<!-- <a href="https://youtu.be/ho4GtXvUphY">[IARAI Fireside Chat]</a> -->
<!-- <br> -->
<!-- <b><u>External Videos:</u></b> -->
<!-- <a href="https://youtu.be/xJrKIPwVwGM">[Paper Explained]</a> -->
<!-- <a href="https://slideslive.com/38940826">[High Performance NLP]</a> -->
<!-- <a href="https://www.bilibili.com/video/BV14h411f7G3/">[BiliBili (Chinese)]</a> -->
<!-- </div> -->
<!-- </li> -->
<!---->
<!-- <li> -->
<!-- <div class="title">An Ode to an ODE</div> -->
<!-- <div>Krzysztof Choromanski<span style="background-position: right top;">*</span>, Jared Quincy Davis<span style="background-position: right top;">*</span>, Valerii Likhosherstov<span style="background-position: right top;">*</span>, Xingyou Song, Jean-Jacques Slotine, Jacob Varley, Honglak Lee, Adrian Weller, Vikas Sindhwani</div> -->
<!-- <div>Neural Information Processing Systems (NeurIPS) 2020</div> -->
<!-- <div> -->
<!-- <a href="https://arxiv.org/abs/2006.11421">[arXiv]</a> -->
<!-- <a href="https://nips.cc/virtual/2020/public/poster_228669109aa3ab1b4ec06b7722efb105.html">[NeurIPS]</a> -->
<!-- </div> -->
<!-- </li> -->
<!---->
<!-- <li> -->
<!-- <div class="title">Rapidly Adaptable Legged Robots via Evolutionary Meta-Learning</div> -->
<!-- <div>Xingyou Song<span style="background-position: right top;">*</span>, Yuxiang Yang<span style="background-position: right top;">*</span>, Krzysztof Choromanski, Ken Caluwaerts, Wenbo Gao, Chelsea Finn, Jie Tan</div> -->
<!-- <div>International Conference on Intelligent Robots and Systems (IROS) 2020</div> -->
<!-- <div> -->
<!-- <b><u>Paper:</u></b> -->
<!-- <a href="https://arxiv.org/abs/2003.01239">[arXiv]</a> -->
<!-- <a href="https://dl.acm.org/doi/abs/10.1109/IROS45743.2020.9341571">[IROS]</a> -->
<!-- &emsp;&emsp;&emsp;&emsp;<b><u>Code:</u></b> -->
<!-- <a href="https://github.com/google-research/google-research/tree/master/es_maml">[ES-MAML Code]</a> -->
<!-- <br> -->
<!-- <b><u>Blogposts:</u></b> -->
<!-- <a href="https://ai.googleblog.com/2020/04/exploring-evolutionary-meta-learning-in.html">[Google AI Blog]</a> -->
<!-- <a href="https://ai.googleblog.com/2021/01/google-research-looking-back-at-2020.html">[Google Research 2020]</a> -->
<!-- <a href="https://pybullet.org/wordpress/index.php/2020/03/17/rapidly-adaptable-legged-robots-via-evolutionary-meta-learning/">[PyBullet Blog]</a> -->
<!-- <br> -->
<!-- <b><u>Videos:</u></b> -->
<!-- <a href="https://www.youtube.com/watch?v=_QPMCDdFC3E">[Experiment Video]</a> -->
<!-- <a href="https://youtu.be/-_GP5ghLy-w">[IROS Presentation Video]</a> -->
<!-- <a href="https://bit.ly/3cROl2p">[AI Frontier Talk Video]</a> -->
<!-- <br> -->
<!-- <b><u>Slides:</u></b> -->
<!-- <a href="data/iros2020_esmaml_ppt.pdf">[IROS 2020 Slides]</a> -->
<!-- <a href="https://rosanneliu.com/dlctfs/dlct_210521.pdf">[ML Collective Slides]</a> -->
<!-- <a href="data/esmaml_aifrontier_talk.pdf">[AI Frontier Slides]</a> -->
<!-- </div> -->
<!-- </li> -->
<!---->
<!-- <li> -->
<!-- <div class="title">Robotic Table Tennis with Model-Free Reinforcement Learning</div> -->
<!-- <div>Wenbo Gao<span style="background-position: right top;">*</span>, Laura Graesser<span style="background-position: right top;">*</span>, Krzysztof Choromanski<span style="background-position: right top;">*</span>, Xingyou Song, Nevena Lazic, Pannag Sanketi, Vikas Sindhwani, Navdeep Jaitly</div> -->
<!-- <div>International Conference on Intelligent Robots and Systems (IROS) 2020</div> -->
<!-- <div> -->
<!-- <a href="https://arxiv.org/abs/2003.14398">[arXiv]</a> -->
<!-- <a href="https://www.youtube.com/watch?v=-eHeq1nvHAE">[Video]</a> -->
<!-- </div> -->
<!-- </li> -->
<!---->
<!-- <li> -->
<!-- <div class="title">Stochastic Flows and Geometric Optimization on the Orthogonal Group</div> -->
<!-- <div>Krzysztof Choromanski<span style="background-position: right top;">*</span>, David Cheikhi<span style="background-position: right top;">*</span>, Jared Davis<span style="background-position: right top;">*</span>, Valerii Likhosherstov<span style="background-position: right top;">*</span>, Achille Nazaret<span style="background-position: right top;">*</span>, Achraf Bahamou<span style="background-position: right top;">*</span>, Xingyou Song<span style="background-position: right top;">*</span>, et al.</div> -->
<!-- <div>International Conference on Machine Learning (ICML) 2020</div> -->
<!-- <div> -->
<!-- <a href="https://arxiv.org/abs/2003.13563">[arXiv]</a> -->
<!-- <a href="https://icml.cc/virtual/2020/poster/5770">[ICML]</a> -->
<!-- </div> -->
<!-- </li> -->
<!---->
<!-- <li> -->
<!-- <div class="title">Observational Overfitting in Reinforcement Learning</div> -->
<!-- <div>Xingyou Song, Yiding Jiang, Stephen Tu, Yilun Du, Behnam Neyshabur</div> -->
<!-- <div>International Conference on Learning Representations (ICLR) 2020</div> -->
<!-- <div> -->
<!-- <a href="https://arxiv.org/abs/1912.02975">[arXiv]</a> -->
<!-- <a href="https://iclr.cc/virtual_2020/poster_HJli2hNKDH.html">[ICLR]</a> -->
<!-- <a href="data/generalization_mysteries.pdf">[Slides]</a> -->
<!-- <a href="data/obsgen_ppt.pdf">[Slides (Old)]</a> -->
<!-- <a href="data/obsgen_poster.pdf">[Poster]</a> -->
<!-- <a href="https://github.com/irom-lab/Invariant-Policy-Optimization/tree/master/LQR">[LQR Code]</a> -->
<!-- </div> -->
<!-- </li> -->
<!---->
<!-- <li> -->
<!-- <div class="title">Gradientless Descent: High-Dimensional Zeroth-Order Optimization</div> -->
<!-- <div>(a) Daniel Golovin, John Karro, Greg Kochanski, Chansoo Lee, Xingyou Song, Qiuyi Zhang</div> -->
<!-- <div>International Conference on Learning Representations (ICLR) 2020 <span style="color:red"><b>(Spotlight, Top 6% of Submissions)</b></span></div> -->
<!-- <div> -->
<!-- <a href="https://arxiv.org/abs/1911.06317">[arXiv]</a> -->
<!-- <a href="https://iclr.cc/virtual_2020/poster_Skep6TVYDB.html">[ICLR]</a> -->
<!-- <a href="data/gradientless_ppt.pdf">[Slides]</a> -->
<!-- </div> -->
<!-- </li> -->
<!---->
<!-- <li> -->
<!-- <div class="title">ES-MAML: Simple Hessian-Free Meta Learning</div> -->
<!-- <div>Xingyou Song<span style="background-position: right top;">*</span>, Wenbo Gao<span style="background-position: right top;">*</span>, Yuxiang Yang, Krzysztof Choromanski, Aldo Pacchiano, Yunhao Tang</div> -->
<!-- <div>International Conference on Learning Representations (ICLR) 2020</div> -->
<!-- <div>Neural Information Processing Systems (NeurIPS) 2019, Workshop on Meta-Learning <b>(Spotlight)</b></div> -->
<!-- <div> -->
<!-- <b><u>Paper:</u></b> -->
<!-- <a href="https://arxiv.org/abs/1910.01215">[arXiv]</a> -->
<!-- <a href="https://iclr.cc/virtual_2020/poster_S1exA2NtDB.html">[ICLR]</a> -->
<!-- <a href="data/esmaml_ppt.pdf">[Slides]</a> -->
<!-- <a href="data/esmaml_poster.pdf">[Poster]</a> -->
<!-- <a href="https://github.com/google-research/google-research/tree/master/es_maml">[Code]</a> -->
<!-- </div> -->
<!-- <div> -->
<!-- <b><u>External Media:</u></b> -->
<!-- <a href="https://www.sohu.com/a/349275764_99979179/">[AITechBaseCamp (Chinese)]</a> -->
<!-- </div> -->
<!-- </li> -->
<!---->
<!-- </ul> -->
<!---->
<!-- <h2>Workshop Papers</h2>  -->
<!-- <ul> -->
<!---->
<!-- <li> -->
<!-- <div class="title">An Empirical Study on Hyperparameters and their Interdependence for RL Generalization</div> -->
<!-- <div>Xingyou Song, Yilun Du, Jacob Jackson</div> -->
<!-- <div>Interntional Conference on Machine Learning (ICML) 2019 Workshop in Understanding and Improving Generalization in Deep Learning</div> -->
<!-- <div> -->
<!-- <a href="https://arxiv.org/abs/1906.00431">[arXiv]</a> -->
<!-- </div> -->
<!-- </li> -->
<!---->
<!-- <li> -->
<!-- <div class="title">The Principle of Unchanged Optimality in Reinforcement Learning Generalization</div> -->
<!-- <div>(a) Alex Irpan, Xingyou Song</div> -->
<!-- <div>Interntional Conference on Machine Learning (ICML) 2019 Workshop in Understanding and Improving Generalization in Deep Learning</div> -->
<!-- <div> -->
<!-- <a href="https://arxiv.org/abs/1906.00336">[arXiv]</a> -->
<!-- </div> -->
<!-- </li> -->
<!---->
<!-- </ul> -->
<!---->
<!---->
<!-- <b><span style="background-position: right top;">(a)</span> denotes alphabetical author ordering.</b> -->
<!---->
<!-- <b><span style="background-position: right top;">*</span> denotes equal contribution.</b> -->
<!---->
<!---->
<!---->
<!-- <iframe id="O2zyMvCB" frameborder="0" src="chrome-extension://ekhagklcjbdpajgpjgmbionohlpdbjgc/translateSandbox/translateSandbox.html" style="width: 0px; height: 0px; display: none;"></iframe><iframe id="n8oRX8Cf" frameborder="0" src="chrome-extension://ekhagklcjbdpajgpjgmbionohlpdbjgc/translateSandbox/translateSandbox.html" style="width: 0px; height: 0px; display: none;"></iframe> -->
<!-- </body>  -->
